from convseq2seq import convseq2seq
from config import opts
import torch
import os

import pdb

pre_model_path = opts.model
if not os.path.exists(pre_model_path):
    print('model file %s not exist, check ur model path' % pre_model_path)
    exit()
model = torch.load(pre_model_path)

device = opts.DEVICE


f = open('s2i.train','r',encoding='utf-8-sig')
dic_train = eval(f.read())
f.close()
'''
dic = {}
for k,v in dic_train.items():
    dic[v] = k
f = open('i2s.train','w',encoding='utf-8-sig')
f.write(str(dic))
f.close()
'''
f = open('s2i.label','r',encoding='utf-8-sig')
dic_label = eval(f.read())
f.close()

'''
dic = {}
for k,v in dic_label.items():
    dic[v] = k
f = open('i2s.label','w',encoding='utf-8-sig')
f.write(str(dic))
f.close()
exit()
'''
def getAction(lst):
    #import pdb
    #pdb.set_trace()
    x = np.array(lst)
    res = np.zeros(51)
    res[left]  = x
    res[right] = x
    return res

def smooth(action):
    alpha = 0
    for i in range(1,action.shape[0]):
        action[i] = alpha * action[i-1] + (1-alpha) * action[i]
    return action

def getGT(txt):
    lst = txt.strip().replace('\n','').split()
    action = np.zeros((len(lst),51))
    th = 0
    for i in lst:
        s = []
        for j in i.split('_'):
            s.append(float(j))
        action[th,left]  = np.array(s)
        action[th,right] = np.array(s)
        th += 1
    res = np.zeros((action.shape[0],54))
    res[:,3:] = action
    return res

test = '好 好 好 好 好 好 好 好 接 接 接 接 接 接 着 着 着 着 刚 刚 刚 刚 刚 刚 刚 刚 刚 刚 那 那 那 那 那 个 个 个 个 排 排 排 排 排 排 排 排 泄 泄 泄 泄 泄 泄 的 的 的 的 话 话 话 话 话 话 话 题 题 题 题 题 题 啊 啊 啊 啊 啊 <s> <s>'
test = '今 今 今 今 今 今 今 天 天 天 我 我 们 们 们 来 来 来 来 聊 聊 聊 聊 聊 一 一 下 下 下 小 小 小 小 小 小 小 狗 狗 狗 狗 狗 狗 <s>'
label = '8.135_7.29844_1.30914_0.0389033_15.6814_2.36959_0.0329591_0.20667 4.46627_4.62592_0.0_0.0388659_17.7273_2.18652_0.115509_0.129943 4.0497_3.7758_0.0349393_0.0392899_17.1265_1.90085_0.332483_0.0706834 4.59403_4.53041_0.066369_0.040144_16.7431_1.56604_0.695526_0.0335179 8.04057_6.67211_0.0944247_0.041397_16.3115_1.23556_1.21628_0.0230728 16.1083_12.7083_0.119242_0.0430177_13.1881_0.962868_1.90638_0.0439758 31.6464_24.2697_0.140956_0.044975_7.59817_0.801431_2.77749_2.09334 44.2844_34.0101_0.159704_0.0472375_5.64779_3.29948_5.39895_3.51593 47.7739_37.1712_0.175619_0.0497741_5.61166_5.3099_6.43173_4.21066 46.5831_36.6193_0.188839_0.0525535_5.76443_5.86056_5.94514_4.07641 45.4699_35.6072_0.199498_0.0555446_6.80702_3.9794_4.00855_3.01209 42.123_32.6659_0.207732_0.0587161_6.19216_3.33447_3.33115_2.59643 31.5562_24.6363_0.213676_0.0620368_5.96847_2.69086_2.15851_1.6928 17.8354_14.5724_0.217467_0.0654756_8.18457_2.07218_0.92371_0.695797 9.99695_8.81214_0.21924_0.069001_11.0673_1.50197_0.0598408_0.0 6.64122_5.82604_0.219131_0.0725821_13.4932_1.00384_0.0_0.0 4.5257_4.08443_0.217274_0.0761875_15.3571_0.601335_0.0230509_0.0164074 3.34056_3.16325_0.213806_0.0797861_16.5539_0.318048_0.0443503_0.0304994 2.94196_2.496_0.208862_0.0833466_16.9781_0.177548_0.0638491_0.0423991 2.5855_2.04161_0.202579_0.0868378_16.6649_0.203411_0.081498_0.0522296 2.27239_1.75899_0.19509_0.0902285_17.0147_0.419209_0.097248_0.060114 2.00379_1.60707_0.186532_0.0934875_17.6923_0.542284_0.11105_0.0661754 1.78089_1.54478_0.177041_0.0965836_18.3631_0.621547_0.122855_0.0705368 1.60488_1.53105_0.166753_0.0994856_18.6921_0.662744_0.132613_0.0733213 1.47694_1.52479_0.155801_0.102162_18.3445_0.671618_0.140276_0.074652 1.39825_1.48494_0.144323_0.104582_16.9853_0.653913_0.145794_0.0746519 1.37_1.37041_0.132455_0.106714_16.8026_0.615375_0.149118_0.0734442 1.39337_1.81655_0.12033_0.108528_16.6843_0.561748_0.150199_0.0711518 1.46954_1.98493_0.108086_0.109991_16.6092_0.498776_0.148988_0.0678979 1.59971_1.96693_0.095857_0.111072_16.5563_0.432204_0.145436_0.0638056 1.78504_1.85395_0.0837795_0.111741_16.5046_0.367775_0.139494_0.0589979 2.02673_1.73737_0.0719888_0.111966_16.433_0.311235_0.131111_0.0535978 2.32596_1.70858_0.0606206_0.111716_16.3204_0.268327_0.120241_0.0477285 2.68391_1.85897_0.0498103_0.11096_16.1459_0.244797_0.106832_0.0415131 3.10177_2.27993_0.0396936_0.109666_15.8882_0.246388_0.0908361_0.0350745 3.58072_3.06285_0.0304062_0.107804_15.5265_0.278846_0.0722042_0.028536 4.12194_4.29912_0.0220835_0.105341_15.0396_0.347913_0.050887_0.0220205 7.09041_6.08013_0.0148611_0.102248_14.4065_0.459336_0.0268353_0.015651 15.4011_12.5627_0.00887467_0.098492_10.8665_0.618858_0.0_0.00955084 33.8724_26.4295_0.00425974_0.0940426_6.63548_0.832224_3.82936_2.40529 46.5969_35.5929_0.00115197_0.510212_5.47677_1.10518_6.5624_4.10819 47.4659_36.1424_0.0_0.729643_6.92884_1.44346_8.33628_5.21011 48.0976_36.7675_0.0_0.648173_7.57845_1.46772_9.28814_5.80295 54.6024_40.0277_3.30273_0.495003_3.67756_1.45481_9.55513_5.97857 59.1846_42.3654_7.51744_0.367708_0.0_1.40114_9.27444_5.82887 54.049_40.223_4.50167_0.264308_3.9455_1.30312_8.5832_5.44573 36.9582_28.9335_9.7972e-07_0.182819_12.6535_1.15715_3.76739_2.45228 24.4399_20.3797_6.60417e-07_0.12126_15.9481_0.959657_1.08101_0.727211 18.0783_14.419_0.0_0.0776473_17.7548_0.70703_0.0_0.0 13.3287_10.9085_0.000666652_0.0499999_18.5321_0.395682_0.0_0.0 11.8645_9.70547_0.0_0.036335_19.0093_0.022021_0.436617_0.281318 11.689_9.46838_0.00520926_0.0346704_19.2325_0.0_0.508679_0.327749 10.8807_8.85626_0.0238851_0.0430238_19.2479_0.177497_0.362412_0.233507 9.78542_8.0981_0.0633641_0.0594127_19.1016_0.495823_0.144043_0.0928093 8.74887_7.42289_0.131046_0.0818552_18.8398_0.895675_0.0_0.0 8.11689_7.05963_0.234331_0.108369_17.8399_1.3179_0.0759015_0.0489045 8.23527_7.2373_0.380619_0.136971_17.0968_1.70336_0.518581_0.334129 10.7325_9.19945_0.577311_0.165679_15.0101_1.99289_1.47406_0.949758 27.014_21.4509_0.831805_0.192512_9.97948_2.12734_3.08857_1.99001 51.5386_38.4626_1.1515_0.215487_4.72856_2.04758_8.59775_5.59301 57.0437_41.3668_1.5438_0.232621_3.11516_1.88239_7.26825_5.11664 53.4433_39.49_2.0161_0.241932_5.43203_1.72667_4.80922_3.74309 43.3702_32.2317_1.62078_0.241438_6.33086_1.58015_2.61529_2.19156 33.8968_26.3111_1.27856_0.229157_7.23162_1.44259_0.930763_0.823656 28.2904_21.7517_0.986136_0.203106_8.12346_1.31375_0.0_0.000976619 24.0567_18.5772_0.740186_0.161304_8.99554_1.19337_0.0671837_0.0851366 21.7507_16.811_0.537397_0.101767_9.83701_1.08121_0.249754_0.16875 21.3882_16.4766_0.374455_0.0225133_10.637_0.977013_0.390699_0.231973 19.9989_15.2023_0.248044_0.171714_11.3847_0.880538_0.493715_0.27668 18.926_14.3932_0.154849_0.433578_12.0693_0.791535_0.562499_0.304745 18.1177_13.9489_0.0915554_0.673957_12.6798_0.709755_0.600746_0.318042 17.5224_13.7693_0.0548482_0.758701_13.2055_0.634949_0.612154_0.318446 17.0881_13.754_0.0414128_0.553657_13.6355_0.566869_0.600419_0.307831 16.7633_13.8027_0.0479336_0.423984_13.959_0.505267_0.569237_0.288072 16.496_13.8154_0.0710963_0.3151_14.165_0.449895_0.522305_0.261044'

lst  = test.strip().replace('\n','').split()
x = []
ppx = torch.LongTensor([len(lst),])
for i in lst:
    if i in dic_train:
        x.append(dic_train[i])
    else:
        x.append(dic_train['<unk>'])


x = torch.LongTensor(x).unsqueeze(0)
px = torch.zeros(x.shape)
mask1 = torch.zeros(x.shape)
for idx in range(x.shape[0]):
    px[idx, :ppx[idx]] = torch.arange(0, ppx[idx])
    mask1[idx, :ppx[idx]] = torch.ones(ppx[idx].item())

#import pdb
#pdb.set_trace()
label = '<S> 1_1_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 2_2_0_0_1_0_0_0 3_2_0_0_1_0_1_0 3_2_0_0_1_1_1_0 3_2_0_0_1_1_1_0 3_2_0_0_1_0_0_0 3_2_0_0_1_0_0_0 2_2_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 0_0_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 2_2_0_0_1_0_0_0 3_2_0_0_1_0_1_0 3_2_0_0_1_0_1_1 3_2_0_0_1_0_1_1 3_3_0_0_0_0_1_1 3_3_1_0_0_0_1_1 3_3_0_0_0_0_1_1 2_2_0_0_1_0_0_0 2_2_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 2_2_0_0_1_0_0_0 3_2_0_0_0_0_1_1 3_3_0_0_0_0_1_1 3_2_0_0_1_0_0_0 3_2_0_0_1_0_0_0 2_2_0_0_1_0_0_0 2_2_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0'
label = '<S> 3_3_0_0_0_0_0_0 3_3_0_0_1_0_0_0 3_2_0_0_1_0_0_0 2_2_0_0_1_0_0_0 2_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 2_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_1_0_0_0 1_1_0_0_2_0_0_0 1_1_0_0_2_0_0_0 1_1_0_0_2_0_0_0 1_1_0_0_2_0_0_0 1_1_0_0_1_0_0_0 2_2_0_0_1_0_0_0 3_2_0_0_0_0_1_0 3_3_0_0_0_1_1_0 3_3_1_0_0_1_1_0 3_2_1_0_0_1_1_1 3_2_1_0_0_1_1_1 2_2_1_0_0_1_1_1 2_1_1_0_0_1_1_1 2_1_1_0_0_1_1_1 1_1_1_0_0_0_1_1 1_1_1_0_0_0_1_0'
y_gt = label.split()
res  = [] 
for i in y_gt:
    res.append(dic_label[i])
y = torch.LongTensor(res).unsqueeze(0)
py = torch.arange(y.shape[1]).long()
x,px,mask1,y,py = x.long().to(device),px.long().to(device),mask1.long().to(device),y.long().to(device),py.long().to(device)
res = model(x,px,y,py,mask1)
_, acc = torch.max(res[0], 1)
print('all in acc:', acc , '\nall in y:',y)

y  = [dic_label['<S>'],]
y  = torch.LongTensor(y).unsqueeze(0)
py = torch.LongTensor([0]).unsqueeze(0)

x,px,mask1,y,py = x.long().to(device),px.long().to(device),mask1.long().to(device),y.long().to(device),py.long().to(device)
final_res = []
for idx in range(x.shape[1]):
    res = model(x,px,y,py,mask1)
    _, acc = torch.max(res[0,idx,], 0) #res[0,idx,] one row
    #print('acc=',acc)
    #pdb.set_trace()
    final_res.append(acc.cpu().item())
    ny  =  acc.unsqueeze(0).unsqueeze(0)
    y   =  torch.cat((y,ny), 1)
    #print('y=', y)
    npy =  torch.LongTensor([idx+1]).unsqueeze(0).to(device)
    py  =  torch.cat((py,npy), 1)
    if idx == x.shape[1] - 1:
        _, acc = torch.max(res[0], 1)
        print('acc all:',acc)
#exit() 
f = open('i2s.label','r',encoding='utf-8-sig')
dic = eval(f.read())
f.close()

import numpy as np
seg   = [5,20,40,70,100]
left  = [0,2,4,8,12,14,16,17]
right = [1,3,5,9,13,15,16,18]
#print(final_res)
final_action = np.zeros((len(final_res),54))
th = 0
for i in final_res:
    #pdb.set_trace()
    #print(i)
    i = dic[i]
    print(i)
    lst = i.split('_')
    ss = []
    for j in lst:
        #pdb.set_trace()
        j = int(j)
        if j == 4:
            res = 100
        elif j == 0:
            res = 0
        else:
            res = (seg[j] + seg[j-1]) / 2.0
        ss.append(res)
    rs = getAction(ss)
    final_action[th,3:] = rs
    th += 1

label = '65.6918_48.8725_0.411112_1.10849_4.83896_3.38537_0.318785_0.652695 59.4345_46.3704_0.264144_1.91862_7.85495_4.10698_0.461733_0.628871 45.634_36.5496_0.304818_2.07387_8.43463_4.53072_0.638081_0.580135 33.7364_27.0711_0.338611_2.22972_7.44249_4.55224_0.850465_0.503577 23.4879_19.2009_0.365894_2.38556_5.61216_3.58112_1.10152_0.396285 19.9245_15.212_0.387041_2.54081_5.771_2.75012_1.39387_0.255351 22.7462_17.3774_0.402423_2.69486_8.8871_2.04968_1.73017_0.625905 24.3289_18.3231_0.412414_2.84714_11.3128_1.47026_2.11303_1.32119 23.0374_17.2228_0.417385_2.99705_10.9274_1.00229_2.5451_1.83475 21.4286_16.0926_0.41771_3.14398_9.86736_0.636216_3.02901_1.66011 19.898_14.9531_0.41376_3.28736_9.26814_0.362493_2.48971_1.322 18.7877_14.1194_0.405909_3.42658_9.14882_0.171561_1.4663_0.773075 18.44_13.9066_0.394529_3.56106_9.52845_0.0538671_0.466983_0.25264 19.9319_15.3865_0.379992_3.6902_10.4261_0.0_0.0_0.0 19.069_14.6317_0.362671_3.8134_11.8608_0.0_0.573564_0.254449 18.2135_13.9034_0.342938_3.93009_13.8515_0.0446612_0.635006_0.315369 19.7273_15.4629_0.321166_4.03965_16.3837_0.12437_0.671265_0.353682 20.8779_16.1495_0.297727_4.1415_18.7267_0.229544_0.684957_0.372037 21.7642_17.3019_0.272994_4.23505_18.2831_0.350627_0.678699_0.373083 22.4217_18.4358_0.24734_4.3197_19.2339_0.478065_0.655108_0.359471 22.8858_19.0666_0.221136_4.39487_19.3279_0.602304_0.6168_0.333849 23.1921_18.71_0.194756_4.45994_18.859_0.713791_0.566393_0.298867 21.7057_16.8816_0.168571_4.51435_18.1212_0.802969_0.506502_0.257175 20.2313_16.3553_0.142955_4.55748_17.4082_0.860282_0.439745_0.211422 18.8413_15.6234_0.118279_4.58875_17.0139_0.87618_0.368738_0.164257 17.6078_14.8_0.0949172_4.60757_17.2323_0.841107_0.296099_0.11833 16.6031_13.9995_0.0732409_4.61334_18.4227_0.745506_0.224444_0.0762911 15.8995_13.336_0.0536229_4.60547_20.1557_0.579826_0.15639_0.0407892 15.5692_12.9238_0.0364357_4.58336_21.5768_0.334508_0.0945534_0.0144736 15.6845_12.8771_0.0220519_4.54643_21.8312_0.0_0.041551_0.0 16.3176_13.3102_0.010844_4.49408_20.0643_1.25338_0.0_0.0 17.5408_14.3373_0.00318428_3.29826_14.3226_2.37408_3.54582_1.7895 33.4012_25.8062_0.0_2.35208_5.01484_3.36839_4.94706_2.9166 48.8894_36.217_0.0_1.62194_2.26961_4.24261_5.1459_3.59467 56.9768_40.209_2.10777_1.07427_1.27406_5.00302_5.08452_4.03711 59.9987_40.9037_5.05674_0.675455_1.00149_5.65593_5.70509_4.45729 50.0764_34.078_8.17064_0.391921_0.752305_6.20763_7.9498_5.06861 43.0862_29.6814_10.7732_0.190076_0.53099_6.6644_12.9829_7.00621 35.3315_24.5401_12.188_0.0363315_0.342047_7.03255_12.0444_6.36145 27.6835_19.3298_11.7389_0.0688491_0.18997_7.14217_12.7437_6.35905 21.0132_14.7261_8.74959_0.0967366_0.0792542_6.3059_13.0745_6.05233 16.1917_11.4048_6.9625_0.120278_0.0143958_4.88658_12.3566_5.52994 14.0901_10.0416_5.42873_0.139759_0.0_3.24702_10.7752_4.88049'
GT = getGT(label)
final_action = smooth(final_action)
np.savetxt('./out/out2.txt',final_action)
np.savetxt('./out/out2_GT.txt',GT)